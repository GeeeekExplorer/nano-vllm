# å®ç°å®Œæˆï¼šM1 + M2 ä¸¤å¡ PD åˆ†ç¦» + GPU1 æµæ°´çº¿

## ğŸ‰ æ€»è§ˆ

å·²å®Œæˆ `DECODE_PIPELINE_PLAN.md` çš„å‰ä¸¤ä¸ªé‡Œç¨‹ç¢‘ï¼š

- âœ… **M1 (é˜¶æ®µä¸€)**ï¼šå¡çº§ Prefill/Decode åˆ†ç¦»
- âœ… **M2 (é˜¶æ®µäºŒ)**ï¼šGPU1 Attention/FFN é™æ€æµæ°´çº¿
- â³ **M3 (é˜¶æ®µä¸‰)**ï¼šåŠ¨æ€è°ƒåº¦ä¸å›é€€ç­–ç•¥ï¼ˆå¾…å®ç°ï¼‰

---

## ğŸ“ æœ€ç»ˆæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          GPU 0 (Prefill Card)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â€¢ Prompt Encoding                                           â”‚  â”‚
â”‚  â”‚  â€¢ KV Cache Generation                                       â”‚  â”‚
â”‚  â”‚  â€¢ Large Batch Prefill                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                              KV Cache Sync
                              (P2P Copy)
                                    â”‚
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU 1 (Decode Card + Pipeline)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Attention Stage (Green Context Stream 1, 16 SMs)           â”‚  â”‚
â”‚  â”‚  â€¢ Token Embedding                                           â”‚  â”‚
â”‚  â”‚  â€¢ KV Cache Lookup                                           â”‚  â”‚
â”‚  â”‚  â€¢ Multi-Head Attention                                      â”‚  â”‚
â”‚  â”‚  â€¢ Input LayerNorm                                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                                                          â”‚
â”‚           â”‚ torch.cuda.Event Synchronization                        â”‚
â”‚           â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  FFN Stage (Green Context Stream 2, 16 SMs)                 â”‚  â”‚
â”‚  â”‚  â€¢ Post-Attention LayerNorm                                  â”‚  â”‚
â”‚  â”‚  â€¢ Feed-Forward Network (MLP)                                â”‚  â”‚
â”‚  â”‚  â€¢ Output LayerNorm                                          â”‚  â”‚
â”‚  â”‚  â€¢ LM Head (Logits Generation)                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ æ ¸å¿ƒç‰¹æ€§

### M1: ä¸¤å¡ PD åˆ†ç¦»
| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| GPU åˆ†å·¥ | GPU0 ä¸“èŒ Prefillï¼ŒGPU1 ä¸“èŒ Decode |
| KV åŒæ­¥ | è‡ªåŠ¨ P2P æ‹·è´ KV cache ä» GPU0 åˆ° GPU1 |
| è°ƒåº¦ç­–ç•¥ | Prefill å®Œæˆååºåˆ—è‡ªåŠ¨è¿ç§»åˆ° GPU1 |
| å¹¶è¡Œæ€§ | GPU0 å¯å¤„ç†æ–° prefillï¼ŒåŒæ—¶ GPU1 æ‰§è¡Œ decode |

### M2: GPU1 æµæ°´çº¿
| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| SM åˆ‡åˆ† | ä½¿ç”¨ Green Context åˆ‡åˆ† GPU1 ä¸º Attention åŒºå’Œ FFN åŒº |
| å¹¶è¡Œæ‰§è¡Œ | Attention å’Œ FFN å¯åœ¨ä¸åŒ token ä¸Šå¹¶è¡Œ |
| äº‹ä»¶åŒæ­¥ | `torch.cuda.Event` ç®¡ç†é˜¶æ®µé—´ä¾èµ– |
| è‡ªåŠ¨é™çº§ | Green Context ä¸å¯ç”¨æ—¶è‡ªåŠ¨å›é€€åˆ°é¡ºåºæ‰§è¡Œ |

---

## ğŸ“Š é…ç½®é€‰é¡¹

### å®Œæ•´é…ç½®ç¤ºä¾‹
```python
from nanovllm import LLM, SamplingParams

llm = LLM(
    "./Qwen3-0.6B/",
    enforce_eager=True,              # M2 éœ€è¦
    tensor_parallel_size=1,          # ä¸¤å¡æ¨¡å¼è¦æ±‚

    # M1 é…ç½®
    enable_two_gpu_pd=True,          # å¯ç”¨ä¸¤å¡ PD åˆ†ç¦»
    prefill_device_id=0,             # Prefill ä½¿ç”¨ GPU0
    decode_device_id=1,              # Decode ä½¿ç”¨ GPU1

    # M2 é…ç½®
    enable_decode_pipeline=True,     # å¯ç”¨ GPU1 æµæ°´çº¿
    decode_attention_sm=16,          # Attention é˜¶æ®µ 16 SMs
    decode_ffn_sm=16,                # FFN é˜¶æ®µ 16 SMs
    decode_pipeline_profiling=True,  # å¯ç”¨æ€§èƒ½ç»Ÿè®¡
)

outputs = llm.generate(
    ["Hello!", "What is AI?"],
    SamplingParams(temperature=0.7, max_tokens=64)
)

# æŸ¥çœ‹ M2 æµæ°´çº¿ç»Ÿè®¡
llm.decode_runner.print_pipeline_statistics()
```

### é…ç½®å±‚çº§
```
å•å¡æ¨¡å¼ (é»˜è®¤)
  â”œâ”€ enable_two_gpu_pd=False
  â””â”€ æ‰€æœ‰æ¨ç†åœ¨å•å¼  GPU ä¸Š

M1 æ¨¡å¼ (ä¸¤å¡ PD åˆ†ç¦»)
  â”œâ”€ enable_two_gpu_pd=True
  â”œâ”€ enable_decode_pipeline=False
  â””â”€ GPU0=Prefill, GPU1=Decode (é¡ºåº)

M1+M2 æ¨¡å¼ (ä¸¤å¡ + æµæ°´çº¿)
  â”œâ”€ enable_two_gpu_pd=True
  â”œâ”€ enable_decode_pipeline=True
  â””â”€ GPU0=Prefill, GPU1=Decode (Attention/FFN å¹¶è¡Œ)
```

---

## ğŸ“ æ–‡ä»¶æ¸…å•

### æ–°å¢æ ¸å¿ƒæ–‡ä»¶
```
M1 é˜¶æ®µ:
  example_two_gpu.py                    # M1 ç¤ºä¾‹
  test_two_gpu.py                       # M1 æµ‹è¯•
  M1_TWO_GPU_PD_SEPARATION.md           # M1 æ–‡æ¡£
  QUICKSTART_TWO_GPU.md                 # M1 å¿«é€Ÿå¼€å§‹
  CHANGELOG_M1.md                       # M1 å˜æ›´æ—¥å¿—

M2 é˜¶æ®µ:
  nanovllm/engine/green_manager.py      # SM èµ„æºç®¡ç†
  nanovllm/engine/pipeline_scheduler.py # ä¸¤é˜¶æ®µè°ƒåº¦å™¨
  example_m2_pipeline.py                # M2 ç¤ºä¾‹
  test_m2_pipeline.py                   # M2 æµ‹è¯•
  M2_DECODE_PIPELINE.md                 # M2 æ–‡æ¡£
  CHANGELOG_M2.md                       # M2 å˜æ›´æ—¥å¿—

å·¥å…·å’ŒåŸºç¡€:
  green_ctx.py                          # Green Context å·¥å…·
  DECODE_PIPELINE_PLAN.md               # åŸå§‹è§„åˆ’æ–‡æ¡£
  IMPLEMENTATION_COMPLETE_M1_M2.md      # æœ¬æ–‡æ¡£
```

### ä¿®æ”¹çš„æ ¸å¿ƒæ–‡ä»¶
```
nanovllm/config.py
  â€¢ M1: +3 lines (enable_two_gpu_pd, prefill_device_id, decode_device_id)
  â€¢ M2: +4 lines (enable_decode_pipeline, decode_attention_sm, decode_ffn_sm, decode_pipeline_profiling)

nanovllm/engine/llm_engine.py
  â€¢ M1: +80 lines (ä¸¤å¡ååŒè°ƒåº¦é€»è¾‘)
  â€¢ M2: +2 lines (is_decode_runner æ ‡å¿—)

nanovllm/engine/model_runner.py
  â€¢ M1: +30 lines (device_id å‚æ•°, KV åŒæ­¥æ¥å£)
  â€¢ M2: +45 lines (pipeline é›†æˆ, ç»Ÿè®¡æ¥å£)

nanovllm/engine/scheduler.py
  â€¢ M1: +15 lines (prefilled åºåˆ—è¿½è¸ª)

nanovllm/models/qwen3.py
  â€¢ M1: -60 lines (æ¸…ç†æ—§å•å¡ pipeline)
  â€¢ M2: +60 lines (Attention/FFN æ‹†åˆ†æ¥å£)
```

---

## ğŸ§ª æµ‹è¯• & éªŒè¯

### è¿è¡Œç¤ºä¾‹
```bash
# M1: ä¸¤å¡ PD åˆ†ç¦»
python example_two_gpu.py

# M1+M2: ä¸¤å¡ + GPU1 æµæ°´çº¿
python example_m2_pipeline.py
```

### è¿è¡Œæµ‹è¯•
```bash
# M1 æµ‹è¯•å¥—ä»¶
python test_two_gpu.py

# M2 æµ‹è¯•å¥—ä»¶
python test_m2_pipeline.py
```

### æµ‹è¯•è¦†ç›–
| æµ‹è¯•é¡¹ | M1 | M2 |
|--------|----|----|
| åŸºç¡€åŠŸèƒ½ | âœ… | âœ… |
| æ­£ç¡®æ€§éªŒè¯ | âœ… | âœ… |
| æ€§èƒ½å¯¹æ¯” | âœ… | âœ… |
| é™çº§å¤„ç† | âœ… | âœ… |

---

## ğŸ“ˆ æ€§èƒ½é¢„æœŸ

### M1 (ä¸¤å¡ PD åˆ†ç¦»)
```
åœºæ™¯: Long prompt (256 tokens)
  å•å¡æ¨¡å¼:   Prefill=200ms, Decode=300ms, Total=500ms
  M1 æ¨¡å¼:    Prefill=200ms (GPU0) || Decode=300ms (GPU1), Total=~350ms
  æå‡:       ~1.4x
```

### M1+M2 (ä¸¤å¡ + æµæ°´çº¿)
```
åœºæ™¯: Decode-heavy (4 prompts Ã— 64 tokens)
  M1 æ¨¡å¼:    Decode=1.2ms/token (Attn+FFN é¡ºåº)
  M1+M2 æ¨¡å¼: Decode=~0.7ms/token (Attn||FFN å¹¶è¡Œ)
  æå‡:       ~1.7x (åœ¨ decode é˜¶æ®µ)

æ€»æå‡: M1+M2 ç›¸å¯¹å•å¡çº¦ 2.0-2.5x (å–å†³äºå·¥ä½œè´Ÿè½½)
```

### å½±å“å› ç´ 
- âœ… **æ­£é¢**ï¼šé•¿ promptã€å¤§ batch sizeã€å¤š token ç”Ÿæˆ
- âš ï¸ **ä¸­æ€§**ï¼šçŸ­ promptã€å° batch size
- âŒ **è´Ÿé¢**ï¼šæçŸ­è¾“å…¥ (< 4 tokens)ã€å•è¯·æ±‚

---

## âš™ï¸ ä¾èµ–è¦æ±‚

### ç¡¬ä»¶
- **GPU æ•°é‡**: è‡³å°‘ 2 å¼  CUDA GPU
- **GPU å‹å·**: æ¨è A100, H100 (80+ SMs)
- **GPU è¿æ¥**: æ¨è NVLink (åŠ é€Ÿ KV ä¼ è¾“)
- **VRAM**: æ¯å¼ å¡è‡³å°‘ 16GB

### è½¯ä»¶
- **CUDA**: 12.4+ (M2 Green Context éœ€è¦)
- **PyTorch**: 2.0+
- **cuda-python**: 12.4+ (M2 éœ€è¦)
  ```bash
  pip install cuda-python
  ```

---

## ğŸ›¡ï¸ é™åˆ¶ & çº¦æŸ

### å½“å‰é™åˆ¶
| é™åˆ¶é¡¹ | è¯´æ˜ | å½±å“ |
|--------|------|------|
| Tensor Parallel | ä¸æ”¯æŒ `tensor_parallel_size > 1` | ä¸èƒ½ä¸ TP å…±å­˜ |
| CUDA Graph | M2 ä¸æ”¯æŒ CUDA Graph | éœ€è¦ `enforce_eager=True` |
| é™æ€ SM é…æ¯” | M2 å›ºå®š 16:16 | ä¸åŒæ¨¡å‹å¯èƒ½æ¬¡ä¼˜ |
| å• token | æµæ°´çº¿æ— æ³•å¹¶è¡Œ | éœ€è¦ decode queue æ·±åº¦ >= 4 |

### å·²çŸ¥é—®é¢˜
1. **KV åŒæ­¥å¼€é”€**: å½“å‰ä½¿ç”¨åŒæ­¥æ‹·è´ï¼Œå¯èƒ½æˆä¸ºç“¶é¢ˆ
2. **å†…å­˜ä½¿ç”¨**: ä¸¤å¼ å¡éƒ½åˆ†é…å®Œæ•´ KV cache (2x å†…å­˜)
3. **Green Context ä¾èµ–**: éœ€è¦ CUDA 12.4+ï¼Œä¸æ”¯æŒæ‰€æœ‰ GPU

---

## ğŸ”„ å…¼å®¹æ€§

### âœ… å‘åå…¼å®¹
- é»˜è®¤è¡Œä¸ºä¸å˜ï¼ˆå•å¡æ¨¡å¼ï¼‰
- æ‰€æœ‰ç°æœ‰ API 100% å…¼å®¹
- M1 å¯ç‹¬ç«‹ä½¿ç”¨ï¼ˆä¸å¯ç”¨ M2ï¼‰

### âœ… æ¸è¿›å¼å¯ç”¨
```python
# 1. åŸºçº¿ï¼šå•å¡
llm = LLM("./model")

# 2. å¯ç”¨ M1ï¼šä¸¤å¡ PD åˆ†ç¦»
llm = LLM("./model", enable_two_gpu_pd=True)

# 3. å¯ç”¨ M1+M2ï¼šä¸¤å¡ + æµæ°´çº¿
llm = LLM("./model", enable_two_gpu_pd=True, enable_decode_pipeline=True)
```

---

## ğŸš§ åç»­å·¥ä½œï¼šM3 é˜¶æ®µ

æ ¹æ® `DECODE_PIPELINE_PLAN.md`ï¼ŒM3 å°†å®ç°ï¼š

### 1. åŠ¨æ€ SM é‡å¹³è¡¡
- æ ¹æ® Attention/FFN å®é™…å»¶è¿ŸåŠ¨æ€è°ƒæ•´ SM é…æ¯”
- ç›®æ ‡ï¼šè‡ªåŠ¨æ‰¾åˆ°æœ€ä¼˜é…æ¯”ï¼ˆä¸å†å›ºå®š 16:16ï¼‰

### 2. GPU0 Back-Pressure
- GPU1 è´Ÿè½½è¿‡é«˜æ—¶ï¼ŒGPU0 è‡ªåŠ¨å‡æ…¢ prefill é€Ÿç‡
- é¿å… KV cache åŒæ­¥é˜Ÿåˆ—å †ç§¯

### 3. æ›´æ·±æµæ°´çº¿
- æ”¯æŒ per-layer æµæ°´çº¿ï¼ˆä¸åªæ˜¯ Attention/FFNï¼‰
- æå‡æµæ°´çº¿æ·±åº¦ï¼Œå¢åŠ å¹¶è¡Œåº¦

### 4. è‡ªé€‚åº”é™çº§
- å¼‚å¸¸æƒ…å†µä¸‹è‡ªåŠ¨åˆ‡æ¢æ¨¡å¼ï¼šM1+M2 â†’ M1 â†’ å•å¡
- ä¿è¯ç³»ç»Ÿç¨³å®šæ€§

---

## ğŸ“š æ–‡æ¡£ç´¢å¼•

### å¿«é€Ÿå¼€å§‹
- `QUICKSTART_TWO_GPU.md` - 3 åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹ M1
- `example_two_gpu.py` - M1 æœ€å°ç¤ºä¾‹
- `example_m2_pipeline.py` - M1+M2 å®Œæ•´ç¤ºä¾‹

### è¯¦ç»†æ–‡æ¡£
- `M1_TWO_GPU_PD_SEPARATION.md` - M1 å®Œæ•´æ–‡æ¡£
- `M2_DECODE_PIPELINE.md` - M2 å®Œæ•´æ–‡æ¡£
- `DECODE_PIPELINE_PLAN.md` - åŸå§‹è®¾è®¡è§„èŒƒ

### å˜æ›´æ—¥å¿—
- `CHANGELOG_M1.md` - M1 å˜æ›´è®°å½•
- `CHANGELOG_M2.md` - M2 å˜æ›´è®°å½•

### æµ‹è¯•
- `test_two_gpu.py` - M1 è‡ªåŠ¨åŒ–æµ‹è¯•
- `test_m2_pipeline.py` - M2 è‡ªåŠ¨åŒ–æµ‹è¯•

---

## ğŸ¯ ä½¿ç”¨å»ºè®®

### ä½•æ—¶ä½¿ç”¨ M1
```python
âœ… é€‚åˆåœºæ™¯:
  â€¢ Long prompts (> 128 tokens)
  â€¢ æ··åˆ Prefill/Decode è´Ÿè½½
  â€¢ éœ€è¦é«˜ååé‡

âš ï¸ ä¸é€‚åˆåœºæ™¯:
  â€¢ æçŸ­ prompt (< 16 tokens)
  â€¢ çº¯ Decode è´Ÿè½½
  â€¢ åªæœ‰ 1 å¼  GPU
```

### ä½•æ—¶ä½¿ç”¨ M1+M2
```python
âœ… é€‚åˆåœºæ™¯:
  â€¢ Decode-heavy workload
  â€¢ Batch size >= 4
  â€¢ ç”Ÿæˆè¾ƒé•¿è¾“å‡º (> 64 tokens)
  â€¢ æœ‰ NVLink å’Œé«˜ç«¯ GPU (A100/H100)

âš ï¸ ä¸é€‚åˆåœºæ™¯:
  â€¢ æå° batch size (1-2)
  â€¢ æçŸ­ç”Ÿæˆ (< 16 tokens)
  â€¢ GPU ä¸æ”¯æŒ Green Context
  â€¢ CUDA ç‰ˆæœ¬ < 12.4
```

---

## âœ… éªŒè¯æ¸…å•

- [x] M1 åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡
- [x] M1 æ­£ç¡®æ€§æµ‹è¯•é€šè¿‡
- [x] M2 åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡
- [x] M2 æ­£ç¡®æ€§æµ‹è¯•é€šè¿‡
- [x] M2 æ€§èƒ½æµ‹è¯•é€šè¿‡
- [x] æ‰€æœ‰æ–‡ä»¶è¯­æ³•æ£€æŸ¥é€šè¿‡
- [x] æ–‡æ¡£å®Œæ•´ä¸”å‡†ç¡®
- [x] ç¤ºä¾‹ä»£ç å¯è¿è¡Œ
- [x] å…¼å®¹æ€§ä¿æŒ

---

## ğŸ™ è‡´è°¢

å®ç°åŸºäº `DECODE_PIPELINE_PLAN.md` çš„ç²¾ç¡®è§„èŒƒï¼Œéµå¾ª M1 â†’ M2 â†’ M3 çš„æ¸è¿›å¼è®¾è®¡ã€‚

---

## ğŸ“ æ”¯æŒ

å¦‚éœ€å¸®åŠ©ï¼š
1. æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£ï¼š`M1_TWO_GPU_PD_SEPARATION.md`, `M2_DECODE_PIPELINE.md`
2. è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š`python test_two_gpu.py`, `python test_m2_pipeline.py`
3. æ£€æŸ¥ç¤ºä¾‹ä»£ç ï¼š`example_two_gpu.py`, `example_m2_pipeline.py`

---

**çŠ¶æ€**: âœ… M1 + M2 å®Œæ•´å®ç°å¹¶æµ‹è¯•é€šè¿‡
**ä¸‹ä¸€æ­¥**: å®ç° M3 åŠ¨æ€è°ƒåº¦ä¸å›é€€ç­–ç•¥
